#!/usr/bin/env python3
"""
æ£€æŸ¥çˆ¬å–ä»»åŠ¡çŠ¶æ€çš„è¯Šæ–­è„šæœ¬
"""
import sys
import asyncio
import json
from pathlib import Path
from datetime import datetime
from typing import Optional

# æ·»åŠ  backend ç›®å½•åˆ°è·¯å¾„
backend_root = Path(__file__).parent.parent
if str(backend_root) not in sys.path:
    sys.path.insert(0, str(backend_root))

from app.database import db
from app.repositories.crawl_task_repository import CrawlTaskRepository


async def check_task_status(task_id: str):
    """æ£€æŸ¥ä»»åŠ¡çŠ¶æ€"""
    print(f"\n{'='*60}")
    print(f"æ£€æŸ¥ä»»åŠ¡: {task_id}")
    print(f"{'='*60}\n")
    
    # è·å–ä»»åŠ¡ä¿¡æ¯
    task = await CrawlTaskRepository.get_task(task_id)
    if not task:
        print(f"âŒ ä»»åŠ¡ {task_id} ä¸å­˜åœ¨äºæ•°æ®åº“ä¸­")
        return
    
    # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
    print("ğŸ“‹ ä»»åŠ¡åŸºæœ¬ä¿¡æ¯:")
    print(f"  åç§°: {task.get('name', 'N/A')}")
    print(f"  æ•°æ®æº: {task.get('data_source', 'N/A')}")
    print(f"  çŠ¶æ€: {task.get('status', 'N/A')}")
    print(f"  åˆ›å»ºæ—¶é—´: {task.get('created_at', 'N/A')}")
    print(f"  å¼€å§‹æ—¶é—´: {task.get('started_at', 'N/A')}")
    print(f"  å®Œæˆæ—¶é—´: {task.get('completed_at', 'N/A')}")
    print(f"  æš‚åœæ—¶é—´: {task.get('paused_at', 'N/A')}")
    print(f"  æ›´æ–°æ—¶é—´: {task.get('updated_at', 'N/A')}")
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("\nâš™ï¸  ä»»åŠ¡é…ç½®:")
    print(f"  èµ·å§‹é¡µ: {task.get('start_page', 'N/A')}")
    print(f"  ç»“æŸé¡µ: {task.get('end_page', 'N/A')}")
    print(f"  æ€»é¡µæ•°: {task.get('total_pages', 'N/A')}")
    print(f"  å·²å®Œæˆé¡µæ•°: {task.get('completed_pages', 0)}")
    print(f"  å½“å‰é¡µ: {task.get('current_page', 'N/A')}")
    print(f"  æ‰¹æ¬¡å¤§å°: {task.get('batch_size', 'N/A')}")
    print(f"  æ¡ˆä¾‹ç±»å‹: {task.get('case_type', 'N/A')}")
    print(f"  æœç´¢å…³é”®è¯: {task.get('search_value', 'N/A')}")
    print(f"  å¯ç”¨æ–­ç‚¹ç»­ä¼ : {task.get('enable_resume', 'N/A')}")
    
    # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
    print("\nğŸ“Š è¿›åº¦ç»Ÿè®¡:")
    print(f"  æ€»çˆ¬å–æ•°: {task.get('total_crawled', 0)}")
    print(f"  æ€»ä¿å­˜æ•°: {task.get('total_saved', 0)}")
    print(f"  æ€»å¤±è´¥æ•°: {task.get('total_failed', 0)}")
    print(f"  å·²ä¿å­˜æ‰¹æ¬¡: {task.get('batches_saved', 0)}")
    
    # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
    print("\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
    print(f"  å¹³å‡é€Ÿåº¦: {task.get('avg_speed', 'N/A')} æ¡ˆä¾‹/åˆ†é’Ÿ")
    print(f"  å¹³å‡å»¶è¿Ÿ: {task.get('avg_delay', 'N/A')} ç§’")
    print(f"  é”™è¯¯ç‡: {task.get('error_rate', 'N/A')}")
    
    # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
    if task.get('error_message'):
        print("\nâŒ é”™è¯¯ä¿¡æ¯:")
        print(f"  é”™è¯¯æ¶ˆæ¯: {task.get('error_message', 'N/A')}")
        if task.get('error_stack'):
            print(f"  é”™è¯¯å †æ ˆ:\n{task.get('error_stack', 'N/A')}")
    
    # æ£€æŸ¥æ–­ç‚¹ç»­ä¼ æ–‡ä»¶
    print("\nğŸ“ æ–­ç‚¹ç»­ä¼ æ–‡ä»¶æ£€æŸ¥:")
    resume_file = backend_root / "data" / "json" / task_id / "crawl_resume.json"
    if resume_file.exists():
        try:
            with open(resume_file, 'r', encoding='utf-8') as f:
                resume_data = json.load(f)
            crawled_ids = resume_data.get('crawled_ids', [])
            total_count = resume_data.get('total_count', 0)
            last_updated = resume_data.get('last_updated', 'N/A')
            
            print(f"  âœ… æ–‡ä»¶å­˜åœ¨")
            print(f"  å·²çˆ¬å–IDæ•°é‡: {len(crawled_ids)}")
            print(f"  æ€»è®¡æ•°: {total_count}")
            print(f"  æœ€åæ›´æ–°: {last_updated}")
            
            # æ£€æŸ¥æ—¶é—´å·®
            try:
                last_update_time = datetime.fromisoformat(last_updated.replace('Z', '+00:00'))
                now = datetime.now(last_update_time.tzinfo)
                time_diff = (now - last_update_time).total_seconds()
                hours_diff = time_diff / 3600
                
                print(f"  è·ç¦»æœ€åæ›´æ–°: {hours_diff:.2f} å°æ—¶")
                
                # å¦‚æœè¶…è¿‡1å°æ—¶æ²¡æœ‰æ›´æ–°ï¼Œä¸”çŠ¶æ€æ˜¯runningï¼Œå¯èƒ½å¡ä½äº†
                if task.get('status') == 'running' and hours_diff > 1:
                    print(f"  âš ï¸  è­¦å‘Š: ä»»åŠ¡çŠ¶æ€ä¸º 'running'ï¼Œä½†å·²è¶…è¿‡ {hours_diff:.2f} å°æ—¶æœªæ›´æ–°ï¼Œå¯èƒ½å·²å¡ä½")
            except Exception as e:
                print(f"  âš ï¸  æ— æ³•è§£ææ—¶é—´: {e}")
        except Exception as e:
            print(f"  âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
    else:
        print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {resume_file}")
    
    # æ£€æŸ¥æ‰¹æ¬¡æ–‡ä»¶
    print("\nğŸ“¦ æ‰¹æ¬¡æ–‡ä»¶æ£€æŸ¥:")
    batch_dir = backend_root / "data" / "json" / task_id
    if batch_dir.exists():
        batch_files = sorted(batch_dir.glob("cases_batch_*.json"))
        print(f"  æ‰¾åˆ° {len(batch_files)} ä¸ªæ‰¹æ¬¡æ–‡ä»¶")
        if batch_files:
            print(f"  ç¬¬ä¸€ä¸ªæ‰¹æ¬¡: {batch_files[0].name}")
            print(f"  æœ€åä¸€ä¸ªæ‰¹æ¬¡: {batch_files[-1].name}")
            
            # æ£€æŸ¥æœ€åä¸€ä¸ªæ‰¹æ¬¡æ–‡ä»¶çš„å¤§å°å’Œä¿®æ”¹æ—¶é—´
            last_batch = batch_files[-1]
            stat = last_batch.stat()
            size_mb = stat.st_size / (1024 * 1024)
            mtime = datetime.fromtimestamp(stat.st_mtime)
            print(f"  æœ€åæ‰¹æ¬¡å¤§å°: {size_mb:.2f} MB")
            print(f"  æœ€åæ‰¹æ¬¡ä¿®æ”¹æ—¶é—´: {mtime}")
    else:
        print(f"  âš ï¸  æ‰¹æ¬¡ç›®å½•ä¸å­˜åœ¨: {batch_dir}")
    
    # è¯Šæ–­å»ºè®®
    print("\nğŸ” è¯Šæ–­å»ºè®®:")
    status = task.get('status', '')
    
    if status == 'running':
        # æ£€æŸ¥æ˜¯å¦çœŸçš„åœ¨è¿è¡Œ
        from app.services.crawl_task_executor import get_executor
        executor = get_executor(task_id)
        if executor:
            print(f"  âœ… æ‰§è¡Œå™¨å­˜åœ¨")
            print(f"    æ˜¯å¦è¿è¡Œä¸­: {executor.is_running}")
            print(f"    æ˜¯å¦æš‚åœ: {executor.is_paused}")
        else:
            print(f"  âš ï¸  æ‰§è¡Œå™¨ä¸å­˜åœ¨ï¼ˆä»»åŠ¡å¯èƒ½å·²åœæ­¢ä½†çŠ¶æ€æœªæ›´æ–°ï¼‰")
            print(f"     å»ºè®®: å°è¯•æ¢å¤ä»»åŠ¡æˆ–æ‰‹åŠ¨æ›´æ–°çŠ¶æ€")
        
        # æ£€æŸ¥è¿›åº¦æ˜¯å¦åœæ»
        if task.get('current_page') is not None and task.get('total_pages') is not None:
            current = task.get('current_page', 0)
            total = task.get('total_pages', 0)
            start = task.get('start_page', 0)
            completed = task.get('completed_pages', 0)
            
            if completed >= total:
                print(f"  âœ… å·²å®Œæˆæ‰€æœ‰é¡µæ•° ({completed}/{total})")
                print(f"     å»ºè®®: ä»»åŠ¡åº”è¯¥å·²å®Œæˆï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨æ›´æ–°çŠ¶æ€ä¸º 'completed'")
            else:
                progress = (completed / total * 100) if total > 0 else 0
                print(f"  è¿›åº¦: {completed}/{total} ({progress:.1f}%)")
    
    elif status == 'paused':
        print(f"  â„¹ï¸  ä»»åŠ¡å·²æš‚åœ")
        print(f"     å»ºè®®: ä½¿ç”¨ /api/v1/crawl-tasks/{task_id}/resume æ¢å¤ä»»åŠ¡")
    
    elif status == 'failed':
        print(f"  âŒ ä»»åŠ¡å·²å¤±è´¥")
        if task.get('error_message'):
            print(f"     é”™è¯¯: {task.get('error_message')}")
        print(f"     å»ºè®®: æ£€æŸ¥é”™è¯¯ä¿¡æ¯ï¼Œä½¿ç”¨ /api/v1/crawl-tasks/{task_id}/retry é‡è¯•")
    
    elif status == 'completed':
        print(f"  âœ… ä»»åŠ¡å·²å®Œæˆ")
    
    elif status == 'pending':
        print(f"  â³ ä»»åŠ¡ç­‰å¾…ä¸­")
        print(f"     å»ºè®®: ä½¿ç”¨ /api/v1/crawl-tasks/{task_id}/start å¯åŠ¨ä»»åŠ¡")
    
    # è·å–æœ€è¿‘çš„æ—¥å¿—
    print("\nğŸ“ æœ€è¿‘æ—¥å¿— (æœ€å¤š10æ¡):")
    logs, total = await CrawlTaskRepository.get_logs(task_id, page=1, page_size=10)
    if logs:
        for log in logs[:10]:
            level = log.get('level', 'INFO')
            message = log.get('message', '')
            created_at = log.get('created_at', '')
            icon = "âœ…" if level == "INFO" else "âš ï¸" if level == "WARNING" else "âŒ"
            print(f"  {icon} [{level}] {created_at}: {message}")
    else:
        print("  æ— æ—¥å¿—è®°å½•")
    
    print(f"\n{'='*60}\n")


async def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python check_task_status.py <task_id>")
        print("ç¤ºä¾‹: python check_task_status.py task_aaefcd6593b84f94")
        sys.exit(1)
    
    task_id = sys.argv[1]
    
    try:
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        await db.connect()
        await check_task_status(task_id)
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        await db.disconnect()


if __name__ == "__main__":
    asyncio.run(main())
